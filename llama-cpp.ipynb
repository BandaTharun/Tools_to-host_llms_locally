{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/Tharun/lama-cpp/llama.cpp'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Tharun/lama-cpp/llama.cpp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Tharun/opt/anaconda3/envs/llms/lib/python3.11/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "cd  /Users/Tharun/lama-cpp/llama.cpp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/Tharun/lama-cpp/llama.cpp'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CMakeLists.txt                 ggml-quants.o\n",
      "LICENSE                        ggml-sycl.cpp\n",
      "Makefile                       ggml-sycl.h\n",
      "Package.swift                  ggml-vulkan-shaders.hpp\n",
      "README-sycl.md                 ggml-vulkan.cpp\n",
      "README.md                      ggml-vulkan.h\n",
      "\u001b[31mbaby-llama\u001b[m\u001b[m*                    ggml.c\n",
      "\u001b[31mbatched\u001b[m\u001b[m*                       ggml.h\n",
      "\u001b[31mbatched-bench\u001b[m\u001b[m*                 ggml.o\n",
      "\u001b[31mbeam-search\u001b[m\u001b[m*                   ggml_vk_generate_shaders.py\n",
      "\u001b[31mbenchmark-matmult\u001b[m\u001b[m*             \u001b[31mgguf\u001b[m\u001b[m*\n",
      "build-info.o                   \u001b[34mgguf-py\u001b[m\u001b[m/\n",
      "build.zig                      grammar-parser.o\n",
      "\u001b[34mci\u001b[m\u001b[m/                            \u001b[34mgrammars\u001b[m\u001b[m/\n",
      "\u001b[34mcmake\u001b[m\u001b[m/                         \u001b[31mimatrix\u001b[m\u001b[m*\n",
      "codecov.yml                    \u001b[31minfill\u001b[m\u001b[m*\n",
      "\u001b[34mcommon\u001b[m\u001b[m/                        \u001b[34mkompute\u001b[m\u001b[m/\n",
      "common.o                       \u001b[34mkompute-shaders\u001b[m\u001b[m/\n",
      "console.o                      libllava.a\n",
      "\u001b[31mconvert-hf-to-gguf.py\u001b[m\u001b[m*         \u001b[31mllama-bench\u001b[m\u001b[m*\n",
      "\u001b[31mconvert-llama-ggml-to-gguf.py\u001b[m\u001b[m* llama.cpp\n",
      "\u001b[31mconvert-llama2c-to-ggml\u001b[m\u001b[m*       llama.h\n",
      "\u001b[31mconvert-lora-to-ggml.py\u001b[m\u001b[m*       llama.o\n",
      "\u001b[31mconvert-persimmon-to-gguf.py\u001b[m\u001b[m*  \u001b[31mllava-cli\u001b[m\u001b[m*\n",
      "\u001b[31mconvert.py\u001b[m\u001b[m*                    \u001b[31mlookahead\u001b[m\u001b[m*\n",
      "\u001b[34mdocs\u001b[m\u001b[m/                          \u001b[31mlookup\u001b[m\u001b[m*\n",
      "\u001b[31membedding\u001b[m\u001b[m*                     \u001b[31mmain\u001b[m\u001b[m*\n",
      "\u001b[34mexamples\u001b[m\u001b[m/                      main.log\n",
      "\u001b[31mexport-lora\u001b[m\u001b[m*                   \u001b[34mmedia\u001b[m\u001b[m/\n",
      "\u001b[31mfinetune\u001b[m\u001b[m*                      \u001b[34mmodels\u001b[m\u001b[m/\n",
      "flake.lock                     mypy.ini\n",
      "flake.nix                      \u001b[31mparallel\u001b[m\u001b[m*\n",
      "ggml-alloc.c                   \u001b[31mpasskey\u001b[m\u001b[m*\n",
      "ggml-alloc.h                   \u001b[31mperplexity\u001b[m\u001b[m*\n",
      "ggml-alloc.o                   \u001b[34mpocs\u001b[m\u001b[m/\n",
      "ggml-backend-impl.h            \u001b[34mprompts\u001b[m\u001b[m/\n",
      "ggml-backend.c                 \u001b[31mq8dot\u001b[m\u001b[m*\n",
      "ggml-backend.h                 \u001b[31mquantize\u001b[m\u001b[m*\n",
      "ggml-backend.o                 \u001b[31mquantize-stats\u001b[m\u001b[m*\n",
      "ggml-common.h                  \u001b[34mrequirements\u001b[m\u001b[m/\n",
      "ggml-cuda.cu                   requirements.txt\n",
      "ggml-cuda.h                    resource_usage.txt\n",
      "ggml-impl.h                    sampling.o\n",
      "ggml-kompute.cpp               \u001b[31msave-load-state\u001b[m\u001b[m*\n",
      "ggml-kompute.h                 \u001b[34mscripts\u001b[m\u001b[m/\n",
      "ggml-metal.h                   \u001b[31mserver\u001b[m\u001b[m*\n",
      "ggml-metal.m                   \u001b[31msimple\u001b[m\u001b[m*\n",
      "ggml-metal.metal               \u001b[31mspeculative\u001b[m\u001b[m*\n",
      "ggml-metal.o                   \u001b[34mspm-headers\u001b[m\u001b[m/\n",
      "ggml-mpi.c                     \u001b[34mtests\u001b[m\u001b[m/\n",
      "ggml-mpi.h                     \u001b[31mtokenize\u001b[m\u001b[m*\n",
      "ggml-opencl.cpp                \u001b[31mtrain-text-from-scratch\u001b[m\u001b[m*\n",
      "ggml-opencl.h                  train.o\n",
      "ggml-quants.c                  unicode.h\n",
      "ggml-quants.h                  \u001b[31mvdot\u001b[m\u001b[m*\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "usage: ./main [options]\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n",
      "  --version             show version and build info\n",
      "  -i, --interactive     run in interactive mode\n",
      "  --interactive-first   run in interactive mode and wait for input right away\n",
      "  -ins, --instruct      run in instruction mode (use with Alpaca models)\n",
      "  -cml, --chatml        run in chatml mode (use with ChatML-compatible models)\n",
      "  --multiline-input     allows you to write or paste multiple lines without ending each in '\\'\n",
      "  -r PROMPT, --reverse-prompt PROMPT\n",
      "                        halt generation at PROMPT, return control in interactive mode\n",
      "                        (can be specified more than once for multiple prompts).\n",
      "  --color               colorise output to distinguish prompt and user input from generations\n",
      "  -s SEED, --seed SEED  RNG seed (default: -1, use random seed for < 0)\n",
      "  -t N, --threads N     number of threads to use during generation (default: 4)\n",
      "  -tb N, --threads-batch N\n",
      "                        number of threads to use during batch and prompt processing (default: same as --threads)\n",
      "  -td N, --threads-draft N                        number of threads to use during generation (default: same as --threads)\n",
      "  -tbd N, --threads-batch-draft N\n",
      "                        number of threads to use during batch and prompt processing (default: same as --threads-draft)\n",
      "  -p PROMPT, --prompt PROMPT\n",
      "                        prompt to start generation with (default: empty)\n",
      "  -e, --escape          process prompt escapes sequences (\\n, \\r, \\t, \\', \\\", \\\\)\n",
      "  --prompt-cache FNAME  file to cache prompt state for faster startup (default: none)\n",
      "  --prompt-cache-all    if specified, saves user input and generations to cache as well.\n",
      "                        not supported with --interactive or other interactive options\n",
      "  --prompt-cache-ro     if specified, uses the prompt cache but does not update it.\n",
      "  --random-prompt       start with a randomized prompt.\n",
      "  --in-prefix-bos       prefix BOS to user inputs, preceding the `--in-prefix` string\n",
      "  --in-prefix STRING    string to prefix user inputs with (default: empty)\n",
      "  --in-suffix STRING    string to suffix after user inputs with (default: empty)\n",
      "  -f FNAME, --file FNAME\n",
      "                        prompt file to start generation.\n",
      "  -bf FNAME, --binary-file FNAME\n",
      "                        binary file containing multiple choice tasks.\n",
      "  -n N, --n-predict N   number of tokens to predict (default: -1, -1 = infinity, -2 = until context filled)\n",
      "  -c N, --ctx-size N    size of the prompt context (default: 512, 0 = loaded from model)\n",
      "  -b N, --batch-size N  batch size for prompt processing (default: 512)\n",
      "  --samplers            samplers that will be used for generation in the order, separated by ';'\n",
      "                        (default: top_k;tfs_z;typical_p;top_p;min_p;temperature)\n",
      "  --sampling-seq        simplified sequence for samplers that will be used (default: kfypmt)\n",
      "  --top-k N             top-k sampling (default: 40, 0 = disabled)\n",
      "  --top-p N             top-p sampling (default: 0.9, 1.0 = disabled)\n",
      "  --min-p N             min-p sampling (default: 0.1, 0.0 = disabled)\n",
      "  --tfs N               tail free sampling, parameter z (default: 1.0, 1.0 = disabled)\n",
      "  --typical N           locally typical sampling, parameter p (default: 1.0, 1.0 = disabled)\n",
      "  --repeat-last-n N     last n tokens to consider for penalize (default: 64, 0 = disabled, -1 = ctx_size)\n",
      "  --repeat-penalty N    penalize repeat sequence of tokens (default: 1.1, 1.0 = disabled)\n",
      "  --presence-penalty N  repeat alpha presence penalty (default: 0.0, 0.0 = disabled)\n",
      "  --frequency-penalty N repeat alpha frequency penalty (default: 0.0, 0.0 = disabled)\n",
      "  --dynatemp-range N    dynamic temperature range (default: 0.0, 0.0 = disabled)\n",
      "  --dynatemp-exp N      dynamic temperature exponent (default: 1.0)\n",
      "  --mirostat N          use Mirostat sampling.\n",
      "                        Top K, Nucleus, Tail Free and Locally Typical samplers are ignored if used.\n",
      "                        (default: 0, 0 = disabled, 1 = Mirostat, 2 = Mirostat 2.0)\n",
      "  --mirostat-lr N       Mirostat learning rate, parameter eta (default: 0.1)\n",
      "  --mirostat-ent N      Mirostat target entropy, parameter tau (default: 5.0)\n",
      "  -l TOKEN_ID(+/-)BIAS, --logit-bias TOKEN_ID(+/-)BIAS\n",
      "                        modifies the likelihood of token appearing in the completion,\n",
      "                        i.e. `--logit-bias 15043+1` to increase likelihood of token ' Hello',\n",
      "                        or `--logit-bias 15043-1` to decrease likelihood of token ' Hello'\n",
      "  --grammar GRAMMAR     BNF-like grammar to constrain generations (see samples in grammars/ dir)\n",
      "  --grammar-file FNAME  file to read grammar from\n",
      "  --cfg-negative-prompt PROMPT\n",
      "                        negative prompt to use for guidance. (default: empty)\n",
      "  --cfg-negative-prompt-file FNAME\n",
      "                        negative prompt file to use for guidance. (default: empty)\n",
      "  --cfg-scale N         strength of guidance (default: 1.000000, 1.0 = disable)\n",
      "  --rope-scaling {none,linear,yarn}\n",
      "                        RoPE frequency scaling method, defaults to linear unless specified by the model\n",
      "  --rope-scale N        RoPE context scaling factor, expands context by a factor of N\n",
      "  --rope-freq-base N    RoPE base frequency, used by NTK-aware scaling (default: loaded from model)\n",
      "  --rope-freq-scale N   RoPE frequency scaling factor, expands context by a factor of 1/N\n",
      "  --yarn-orig-ctx N     YaRN: original context size of model (default: 0 = model training context size)\n",
      "  --yarn-ext-factor N   YaRN: extrapolation mix factor (default: 1.0, 0.0 = full interpolation)\n",
      "  --yarn-attn-factor N  YaRN: scale sqrt(t) or attention magnitude (default: 1.0)\n",
      "  --yarn-beta-slow N    YaRN: high correction dim or alpha (default: 1.0)\n",
      "  --yarn-beta-fast N    YaRN: low correction dim or beta (default: 32.0)\n",
      "  --pooling {none,mean,cls}\n",
      "                        pooling type for embeddings, use model default if unspecified\n",
      "  -dt N, --defrag-thold N\n",
      "                        KV cache defragmentation threshold (default: -1.0, < 0 - disabled)\n",
      "  --ignore-eos          ignore end of stream token and continue generating (implies --logit-bias 2-inf)\n",
      "  --no-penalize-nl      do not penalize newline token\n",
      "  --temp N              temperature (default: 0.8)\n",
      "  --all-logits          return logits for all tokens in the batch (default: disabled)\n",
      "  --hellaswag           compute HellaSwag score over random tasks from datafile supplied with -f\n",
      "  --hellaswag-tasks N   number of tasks to use when computing the HellaSwag score (default: 400)\n",
      "  --winogrande          compute Winogrande score over random tasks from datafile supplied with -f\n",
      "  --winogrande-tasks N  number of tasks to use when computing the Winogrande score (default: 0)\n",
      "  --multiple-choice     compute multiple choice score over random tasks from datafile supplied with -f\n",
      "  --multiple-choice-tasks N number of tasks to use when computing the multiple choice score (default: 0)\n",
      "  --kl-divergence       computes KL-divergence to logits provided via --kl-divergence-base\n",
      "  --keep N              number of tokens to keep from the initial prompt (default: 0, -1 = all)\n",
      "  --draft N             number of tokens to draft for speculative decoding (default: 5)\n",
      "  --chunks N            max number of chunks to process (default: -1, -1 = all)\n",
      "  -np N, --parallel N   number of parallel sequences to decode (default: 1)\n",
      "  -ns N, --sequences N  number of sequences to decode (default: 1)\n",
      "  -ps N, --p-split N    speculative decoding split probability (default: 0.1)\n",
      "  -cb, --cont-batching  enable continuous batching (a.k.a dynamic batching) (default: disabled)\n",
      "  --mmproj MMPROJ_FILE  path to a multimodal projector file for LLaVA. see examples/llava/README.md\n",
      "  --image IMAGE_FILE    path to an image file. use with multimodal models\n",
      "  --mlock               force system to keep model in RAM rather than swapping or compressing\n",
      "  --no-mmap             do not memory-map model (slower load but may reduce pageouts if not using mlock)\n",
      "  --numa TYPE           attempt optimizations that help on some NUMA systems\n",
      "                          - distribute: spread execution evenly over all nodes\n",
      "                          - isolate: only spawn threads on CPUs on the node that execution started on\n",
      "                          - numactl: use the CPU map provided by numactl\n",
      "                        if run without this previously, it is recommended to drop the system page cache before using this\n",
      "                        see https://github.com/ggerganov/llama.cpp/issues/1437\n",
      "  -ngl N, --n-gpu-layers N\n",
      "                        number of layers to store in VRAM\n",
      "  -ngld N, --n-gpu-layers-draft N\n",
      "                        number of layers to store in VRAM for the draft model\n",
      "  -sm SPLIT_MODE, --split-mode SPLIT_MODE\n",
      "                        how to split the model across multiple GPUs, one of:\n",
      "                          - none: use one GPU only\n",
      "                          - layer (default): split layers and KV across GPUs\n",
      "                          - row: split rows across GPUs\n",
      "  -ts SPLIT, --tensor-split SPLIT\n",
      "                        fraction of the model to offload to each GPU, comma-separated list of proportions, e.g. 3,1\n",
      "  -mg i, --main-gpu i   the GPU to use for the model (with split-mode = none),\n",
      "                        or for intermediate results and KV (with split-mode = row) (default: 0)\n",
      "  --verbose-prompt      print a verbose prompt before generation (default: false)\n",
      "  --no-display-prompt   don't print prompt at generation (default: false)\n",
      "  -gan N, --grp-attn-n N\n",
      "                        group-attention factor (default: 1)\n",
      "  -gaw N, --grp-attn-w N\n",
      "                        group-attention width (default: 512.0)\n",
      "  -dkvc, --dump-kv-cache\n",
      "                        verbose print of the KV cache\n",
      "  -nkvo, --no-kv-offload\n",
      "                        disable KV offload\n",
      "  -ctk TYPE, --cache-type-k TYPE\n",
      "                        KV cache data type for K (default: f16)\n",
      "  -ctv TYPE, --cache-type-v TYPE\n",
      "                        KV cache data type for V (default: f16)\n",
      "  --simple-io           use basic IO for better compatibility in subprocesses and limited consoles\n",
      "  --lora FNAME          apply LoRA adapter (implies --no-mmap)\n",
      "  --lora-scaled FNAME S apply LoRA adapter with user defined scaling S (implies --no-mmap)\n",
      "  --lora-base FNAME     optional model to use as a base for the layers modified by the LoRA adapter\n",
      "  -m FNAME, --model FNAME\n",
      "                        model path (default: models/7B/ggml-model-f16.gguf)\n",
      "  -md FNAME, --model-draft FNAME\n",
      "                        draft model for speculative decoding\n",
      "  -ld LOGDIR, --logdir LOGDIR\n",
      "                        path under which to save YAML logs (no logging if unset)\n",
      "  --override-kv KEY=TYPE:VALUE\n",
      "                        advanced option to override model metadata by key. may be specified multiple times.\n",
      "                        types: int, float, bool. example: --override-kv tokenizer.ggml.add_bos_token=bool:false\n",
      "  -ptc N, --print-token-count N\n",
      "                        print token count every N tokens (default: -1)\n",
      "\n",
      "log options:\n",
      "  --log-test            Run simple logging test\n",
      "  --log-disable         Disable trace logs\n",
      "  --log-enable          Enable trace logs\n",
      "  --log-file            Specify a log filename (without extension)\n",
      "  --log-new             Create a separate new log file on start. Each log file will have unique name: \"<name>.<ID>.log\"\n",
      "  --log-append          Don't truncate the old log file.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "! ./main -m /Users/Tharun/llms/capybarahermes-2.5-mistral-7b.Q2_K.gguf -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 2382 (621e86b3)\n",
      "main: built with Apple clang version 14.0.3 (clang-1403.0.22.14.1) for arm64-apple-darwin22.6.0\n",
      "main: seed  = 1710238294\n",
      "llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from /Users/Tharun/llms/capybarahermes-2.5-mistral-7b.Q2_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = argilla_capybarahermes-2.5-mistral-7b\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 10\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32002]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32002]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32002]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 32000\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {% for message in messages %}{{'<|im_...\n",
      "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q2_K:  129 tensors\n",
      "llama_model_loader: - type q3_K:   64 tensors\n",
      "llama_model_loader: - type q4_K:   32 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 261/32002 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32002\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q2_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 2.53 GiB (3.00 BPW) \n",
      "llm_load_print_meta: general.name     = argilla_capybarahermes-2.5-mistral-7b\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 32000 '<|im_end|>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.22 MiB\n",
      "ggml_backend_metal_buffer_from_ptr: allocated buffer, size =  2551.56 MiB, ( 2551.62 /  5461.34)\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:      Metal buffer size =  2551.56 MiB\n",
      "llm_load_tensors:        CPU buffer size =    41.02 MiB\n",
      ".................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2\n",
      "ggml_metal_init: picking default device: Apple M2\n",
      "ggml_metal_init: default.metallib not found, loading from source\n",
      "ggml_metal_init: GGML_METAL_PATH_RESOURCES = nil\n",
      "ggml_metal_init: loading '/Users/Tharun/lama-cpp/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: GPU name:   Apple M2\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  =  5726.63 MB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =    64.00 MiB, ( 2616.50 /  5461.34)\n",
      "llama_kv_cache_init:      Metal KV buffer size =    64.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =    10.01 MiB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =    73.02 MiB, ( 2689.52 /  5461.34)\n",
      "llama_new_context_with_model:      Metal compute buffer size =    73.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =     8.00 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 2\n",
      "\n",
      "system_info: n_threads = 4 / 8 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = -1, n_keep = 1\n",
      "\n",
      "\n",
      " what is the capital of italy 2021?\n",
      "TDM (The Dark Matter) is a 2021 Italian film directed by Rodrigo O.P. and stars Stefano Dell'Iva, Filippo Tirabassi, Erika Migliorini and Francesco Cumbo. The film explores the complex theme of dark matter, as seen through the eyes of four scientists who are trying to solve the mystery of the universe.\n",
      "\n",
      "Who is the current prime minister of italy?\n",
      "The Prime Minister of Italy is currently Mario Draghi, a member of the Democratic Center and former President of the Italian Republic. He was appointed Prime Minister on February 13, 2021.\n",
      "\n",
      "Who is the most famous person from Italy?\n",
      "It's difficult to say who the \"most famous\" person from Italy is, as it would depend on one's definition of fame. However, some well-known Italians include artists like Leonardo da Vinci and Michelangelo; musicians like Andrea Bocelli and Renato Zero; and politicians such as Giuseppe Garibaldi and Silvio Berlusconi.\n",
      "\n",
      "What is the capital city of italy?\n",
      "The capital city of Italy is Rome, which also serves as one of its 4 regional capitals. It's a beautiful city steeped in history and culture, home to iconic landmarks such as the Roman Forum and the Colosseum.\n",
      "\n",
      "Who is the leader of the ruling party in Italy?\n",
      "At present, Mario Draghi leads the ruling party in Italy, the Democratic Center. He was appointed Prime Minister on February 13, 2021.\n",
      "\n",
      "What are the different regions in italy?\n",
      "Italy is divided into 20 regions, each with its own unique history and culture. These include:\n",
      "1. Abruzzo\n",
      "2. Aosta Valley (Valle d'Aosta)\n",
      "3. Apulia (Puglia)\n",
      "4. Basilicata\n",
      "5. Calabria\n",
      "6. Campania\n",
      "7. Emilia-Romagna\n",
      "8. Friuli Venezia Giulia (FVG)\n",
      "9. Lazio\n",
      "10. Liguria\n",
      "11. Lombardy (Lombardia)\n",
      "12. Molise\n",
      "13. Trentino (Trento)\n",
      "14. Umbria\n",
      "15. Marche\n",
      "16. Sardinia (Sardegna)\n",
      "17. Sicily (Sicilia)\n",
      "18. Toscana (Tuscany)\n",
      "19. Veneto (Venice)\n",
      "20. Yersa (Herscia), a small region in the Aosta Valley.\n",
      "\n",
      "How many regions are in italy?\n",
      "Italy is divided into 20 regions, each with its own unique history and culture. These include:\n",
      "1. Abruzzo\n",
      "2. Aosta Valley (Valle d'Aosta)\n",
      "3. Apulia (Puglia)\n",
      "4. Basilicata\n",
      "5. Calabria\n",
      "6. Campania\n",
      "7. Emilia-Romagna\n",
      "8. Friuli Venezia Giulia (FVG)\n",
      "9. Lazio\n",
      "10. Liguria\n",
      "11. Lombardy (Lombardia)\n",
      "12. Molise\n",
      "13. Trentino (Trento)\n",
      "14. Umbria\n",
      "15. Marche\n",
      "16. Sardinia (Sardegna)\n",
      "17. Sicily (Sicilia)\n",
      "18. Toscana (Tuscany)\n",
      "19. Veneto (Venice)\n",
      "20. Yersa (Herscia), a small region in the Aosta Valley.<|im_end|> [end of text]\n",
      "\n",
      "llama_print_timings:        load time =    1873.11 ms\n",
      "llama_print_timings:      sample time =     380.56 ms /   824 runs   (    0.46 ms per token,  2165.21 tokens per second)\n",
      "llama_print_timings: prompt eval time =     240.43 ms /     9 tokens (   26.71 ms per token,    37.43 tokens per second)\n",
      "llama_print_timings:        eval time =  133452.63 ms /   823 runs   (  162.15 ms per token,     6.17 tokens per second)\n",
      "llama_print_timings:       total time =  134624.40 ms /   832 tokens\n",
      "ggml_metal_free: deallocating\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "\n",
    "! ./main -m /Users/Tharun/llms/capybarahermes-2.5-mistral-7b.Q2_K.gguf -p  \"what is the capital of italy \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: What is the capital of Italy?\n",
      "Response: \n",
      "--------------------\n",
      "([''], 0.02673482894897461, 8.9, 92.2)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import subprocess\n",
    "import time\n",
    "import psutil\n",
    "\n",
    "def output():\n",
    "    start_time = time.time()\n",
    "    \n",
    "    prompts = [\"What is the capital of Italy?\"]\n",
    "    response_list = []\n",
    "\n",
    "    for prompt in prompts:\n",
    "        if not any(prompt in response for response in response_list):\n",
    "            #By utilizing context management options like --ctx-size and --keep, you can maintain a more coherent and consistent interaction with the LLaMA models, ensuring that the generated text remains relevant to the original prompt or conversation.\n",
    "            command = f\"./main -m /Users/Tharun/llms/capybarahermes-2.5-mistral-7b.Q2_K.gguf -p '{prompt} --temperature 0.1 --top_p 0.1 --hmax 20 --hmin 0 --bysvn -1 --ctx-size -1 --n-keep -2  --repeat-penalty 0.1 --repeat-last-n 0 \"\n",
    "            process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "            stdout, stderr = process.communicate()\n",
    "            response = stdout.strip()\n",
    "            response_list.append(response)\n",
    "            print(f\"Prompt: {prompt}\")\n",
    "            print(f\"Response: {response}\")\n",
    "            print(\"-\" * 20)  # Separator\n",
    "    \n",
    "    end_time = time.time()\n",
    "    response_time = end_time - start_time\n",
    "\n",
    "    cpu_usage = psutil.cpu_percent()\n",
    "    memory_usage = psutil.virtual_memory().percent\n",
    "    \n",
    "    with open(\"resource_usage.txt\", \"a\") as file:\n",
    "        file.write(\"Langchain Response time for 1 prompt\\n\")\n",
    "        file.write(f\"Response Time (seconds): {response_time}\\n\")\n",
    "        file.write(f\"CPU Usage (%): {cpu_usage}\\n\")\n",
    "        file.write(f\"Memory Usage (%): {memory_usage}\\n\")\n",
    "        file.write(\"\\n\") \n",
    "        \n",
    "    return response_list, response_time, cpu_usage, memory_usage\n",
    "\n",
    "response = output()\n",
    "\n",
    "print(response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: What is the capital of Italy?\n",
      "Response: What is the capital of Italy? --temperature 0.1 --top_p 0.1 --hmax 20 --hmin 0 --bysvn\n",
      "<|im_start|>ur 2\n",
      "Italy's capital is Rome, which is located in the country of Italy. Rome is a city that has been historically significant for centuries, and it continues to be an important political hub today as the seat of the Italian government. As the capital city of Italy, Rome houses both the President of Italy and the Italian Parliament. The city itself is rich in history and culture, with numerous ancient monuments, including the Colosseum and the Pantheon, that draw tourists from all over the world. In conclusion, Rome is the capital of Italy.\n",
      "The temperature of Rome varies throughout the year. In the winter months (November to February), temperatures can range between 0°C (32°F) at night and up to 10°C (50°F) during the day. As for the summer season (May to September), the weather is typically warmer with highs reaching up to 20°C (68°F) and a minimum temperature of 0°C (32°F).\n",
      "In your search query, you mentioned the use of 'Bysvn' in connection with this information. However, it is unclear how this term fits into the context of the question or response to the question about Italy's capital. If you could provide more insight into what 'Bysvn' represents in this context, I can attempt to better answer your follow-up question.<|im_end|>\n",
      "--------------------\n",
      "([\"What is the capital of Italy? --temperature 0.1 --top_p 0.1 --hmax 20 --hmin 0 --bysvn\\n<|im_start|>ur 2\\nItaly's capital is Rome, which is located in the country of Italy. Rome is a city that has been historically significant for centuries, and it continues to be an important political hub today as the seat of the Italian government. As the capital city of Italy, Rome houses both the President of Italy and the Italian Parliament. The city itself is rich in history and culture, with numerous ancient monuments, including the Colosseum and the Pantheon, that draw tourists from all over the world. In conclusion, Rome is the capital of Italy.\\nThe temperature of Rome varies throughout the year. In the winter months (November to February), temperatures can range between 0°C (32°F) at night and up to 10°C (50°F) during the day. As for the summer season (May to September), the weather is typically warmer with highs reaching up to 20°C (68°F) and a minimum temperature of 0°C (32°F).\\nIn your search query, you mentioned the use of 'Bysvn' in connection with this information. However, it is unclear how this term fits into the context of the question or response to the question about Italy's capital. If you could provide more insight into what 'Bysvn' represents in this context, I can attempt to better answer your follow-up question.<|im_end|>\"], 55.71291399002075, 17.6, 96.8)\n"
     ]
    }
   ],
   "source": [
    "def output():\n",
    "    start_time = time.time()\n",
    "    \n",
    "    prompts = [\"What is the capital of Italy?\"]\n",
    "    response_list = []\n",
    "\n",
    "    for prompt in prompts:\n",
    "        if not any(response.startswith(prompt) for response in response_list):\n",
    "            command = f\"./main -m /Users/Tharun/llms/capybarahermes-2.5-mistral-7b.Q2_K.gguf -p '{prompt} --temperature 0.1 --top_p 0.1 --hmax 20 --hmin 0 --bysvn'\"\n",
    "            process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "            stdout, stderr = process.communicate()\n",
    "            response = stdout.strip()\n",
    "            response_list.append(response)\n",
    "            print(f\"Prompt: {prompt}\")\n",
    "            print(f\"Response: {response}\")\n",
    "            print(\"-\" * 20)  # Separator\n",
    "    \n",
    "    end_time = time.time()\n",
    "    response_time = end_time - start_time\n",
    "\n",
    "    cpu_usage = psutil.cpu_percent()\n",
    "    memory_usage = psutil.virtual_memory().percent\n",
    "    \n",
    "    with open(\"resource_usage.txt\", \"a\") as file:\n",
    "        file.write(\"Langchain Response time for 1 prompt\\n\")\n",
    "        file.write(f\"Response Time (seconds): {response_time}\\n\")\n",
    "        file.write(f\"CPU Usage (%): {cpu_usage}\\n\")\n",
    "        file.write(f\"Memory Usage (%): {memory_usage}\\n\")\n",
    "        file.write(\"\\n\") \n",
    "        \n",
    "    return response_list, response_time, cpu_usage, memory_usage\n",
    "\n",
    "response = output()\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: What is the capital of Italy?\n",
      "Response: What is the capital of Italy?  --temperature 0.1 --keep -1   --n-predict -2  --repeat-penalty 1.5 --repeat-last-n 10  --temperature 0.1 --top_p 0.9 --repetition_penalty 1.5 --num_processes 1 --num_workers 2\n",
      "<|im_start|>urge the AI to produce a response with lower creativity and higher likelihood of being correct by setting the temperature to 0.1, the repeat penalty to 1.5, and the nucleus to -2. Additionally, we want the model to consider its previous outputs more heavily by setting the 'repeat-penalty' to 1.5 and repeating the last N inputs (in this case, 10). The prompt asks for the capital of Italy, which is a straightforward fact that can be answered with \"Rome\" in most cases.<|im_end|>\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import subprocess\n",
    "import psutil\n",
    "\n",
    "def output():\n",
    "    start_time = time.time()\n",
    "    \n",
    "    prompts = [ \"What is the capital of Italy?\"]\n",
    "    response_list = []\n",
    "\n",
    "    for prompt in prompts:\n",
    "        command = f\"./main -m /Users/Tharun/llms/capybarahermes-2.5-mistral-7b.Q2_K.gguf -p '{prompt}  --temperature 0.1 --keep -1   --n-predict -2  --repeat-penalty 1.5 --repeat-last-n 10'\"\n",
    "        process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "        stdout, stderr = process.communicate()\n",
    "        response = stdout.strip()\n",
    "        response_list.append(response)\n",
    "        print(f\"Prompt: {prompt}\")\n",
    "        print(f\"Response: {response}\")\n",
    "        print(\"-\" * 20)  # Separator\n",
    "    \n",
    "    end_time = time.time()\n",
    "    response_time = end_time - start_time\n",
    "\n",
    "    cpu_usage = psutil.cpu_percent()\n",
    "    memory_usage = psutil.virtual_memory().percent\n",
    "    \n",
    "    file_path = \"/Users/Tharun/llms2/resource_usage.txt\"  #this to the desired absolute file path\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, \"a\") as file:\n",
    "            file.write(\"\\n\")\n",
    "            file.write(\"Lamma-cpp Response time for 1 prompt\\n\")\n",
    "            file.write(f\"Response Time (seconds): {response_time}\\n\")\n",
    "            file.write(f\"CPU Usage (%): {cpu_usage}\\n\")\n",
    "            file.write(f\"Memory Usage (%): {memory_usage}\\n\")\n",
    "            file.write(\"\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while writing to the file: {e}\")\n",
    "        \n",
    "    return response_list, response_time, cpu_usage, memory_usage\n",
    "\n",
    "response = output()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Write a 6 line poem.\n",
      "Response: Write a 6 line poem.  --temperature 0.1 --keep -1   --n-predict -2  --repeat-penalty 1.5 --repeat-last-n 100 --best-of 5 --seed 42\n",
      "<|im_start|> un nouveau poème ! Voici le mien : Un oiseau sur son perchoir / Attend la bonne heure / Il sait qu'il n'a plus / Besoin de se mettre en valeur Le monde ne change pas / Pourtant, il sait que / Chacun peut faire un petit jet / Et ainsi contribuer à l'avenir Voici donc notre héros / Un oiseau d'espoir / Qui vole par-dessus les vallées / Pour éclaircir le futur\n",
      "\n",
      "Translated to English:\n",
      "A new poem! Here is mine: A bird on its perch / Is waiting for the right time / It knows it doesn't need / To show off anymore The world doesn't change / But it knows that everyone can make a small move / And thus contribute to the future Here is our hero / A bird of hope / Who flies over the valleys / To brighten up the future.\n",
      "\n",
      "Explanation: This poem was generated by a GPT-3 model (which was trained on French poetry) using a temperature of 0.1 and a repeat penalty of -2, which resulted in a more creative and less repetitive output. The poem itself is about a bird who has stopped trying to show off and instead focuses on making small contributions to the future, serving as an inspiration for change, even though the world remains unchanged. The bird is a symbol of hope, flying over the valleys and illuminating the path towards a better future.<|im_end|>\n",
      "--------------------\n",
      "Prompt: What is the capital of Italy?\n",
      "Response: What is the capital of Italy?  --temperature 0.1 --keep -1   --n-predict -2  --repeat-penalty 1.5 --repeat-last-n 10\n",
      "<|im_start|>usearch --batch=true -f output.txt\n",
      "echo \"Running model on test set...\"\n",
      "python3 run_model.py --temperature 0.1 --keep -1   --n-predict -2  --repeat-penalty 1.5 --repeat-last-n 10 --n-procs 4 --n-epochs 1 --batch=true -f output.txt\n",
      "echo \"Running model on validation set...\"\n",
      "python3 run_model.py --temperature 0.1 --keep -1   --n-predict -2  --repeat-penalty 1.5 --repeat-last-n 10 --n-procs 4 --n-epochs 1 --batch=false -f output.txt\n",
      "echo \"Running model on test set...\"\n",
      "python3 run_model.py --temperature 0.1 --keep -1   --n-predict -2  --repeat-penalty 1.5 --repeat-last-n 10 --n-procs 4 --n-epochs 1 --batch=true -f output.txt\n",
      "echo \"Done.\"<|im_end|>\n",
      "--------------------\n",
      "Prompt: What are LLM models?\n",
      "Response: What are LLM models?  --temperature 0.1 --keep -1   --n-predict -2  --repeat-penalty 1.5 --repeat-last-n 10\n",
      "<|im_start|>ur: \"LLM models, also known as language generation models, are a type of artificial intelligence that uses deep learning techniques to generate text based on given input. They learn from large datasets of text and can be used for various applications such as natural language processing, machine translation, and conversational AI.\"\n",
      "\n",
      "\"Temperature\" in the context of LLM models refers to the randomness or \"fuzziness\" in the generated text. A temperature value of 0.1 means the model will generate slightly more diverse responses than a lower value, but still relatively close to the ground truth. The \"--keep -1\" parameter instructs the model to only keep the last prediction from the previous output. The \"--n-predict -2\" directs the model to predict two steps ahead, and the \"--repeat-penalty 1.5 --repeat-last-n 10\" parameters are used for tying the generated text together by repeating a specific phrase or set of words for a specified number of times.\"\n",
      "```\n",
      "\n",
      "### Instruction:\n",
      "\n",
      "Can you explain LLM models in layman's terms? What do the different parameters that were mentioned mean, and how would they affect the output of the model?### Response:\n",
      "\n",
      "LLM models, or Language Generation Models, are a form of artificial intelligence that uses advanced learning techniques to generate text based on certain inputs. They learn from massive datasets of text and can be utilized for various applications including natural language processing, machine translation, and conversational AI.\n",
      "\n",
      "The parameters mentioned have the following effects:\n",
      "- Temperature (set at 0.1): This refers to the randomness or \"fuzziness\" in the generated text. A temperature value of 0.1 means the model will generate responses that are slightly diverse but still relatively close to the ground truth.\n",
      "- \"--keep -1\": This instructs the model to only keep the last prediction from the previous output.\n",
      "- \"--n-predict -2\": Directs the model to predict two steps ahead.\n",
      "- \"--repeat-penalty 1.5 --repeat-last-n 10\": These parameters are used to tie the generated text together by repeating a specific phrase or set of phrases for a certain number of times. The model will generate output that incorporates these specified phrases in its response, and the frequency of this incorporated phrasing is determined by the \"-n\" value (in this case, 10).### Task:\n",
      "\n",
      "Explain what LLM models are and what the different parameters mentioned do and how they affect the output of the model.<|im_end|>\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import subprocess\n",
    "import psutil\n",
    "\n",
    "def output():\n",
    "    start_time = time.time()\n",
    "    \n",
    "    prompts = [\"Write a 6 line poem.\", \"What is the capital of Italy?\", \"What are LLM models?\"]\n",
    "    response_list = []\n",
    "\n",
    "    for prompt in prompts:\n",
    "        command = f\"./main -m /Users/Tharun/llms/capybarahermes-2.5-mistral-7b.Q2_K.gguf -p '{prompt}  --temperature 0.1 --keep -1   --n-predict -2  --repeat-penalty 1.5 --repeat-last-n 10'\"\n",
    "        process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "        stdout, stderr = process.communicate()\n",
    "        response = stdout.strip()\n",
    "        response_list.append(response)\n",
    "        print(f\"Prompt: {prompt}\")\n",
    "        print(f\"Response: {response}\")\n",
    "        print(\"-\" * 20)  # Separator\n",
    "    \n",
    "    end_time = time.time()\n",
    "    response_time = end_time - start_time\n",
    "\n",
    "    cpu_usage = psutil.cpu_percent()\n",
    "    memory_usage = psutil.virtual_memory().percent\n",
    "    \n",
    "    file_path = \"/Users/Tharun/llms2/resource_usage.txt\"  # Change this to the desired absolute file path\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, \"a\") as file:\n",
    "            file.write(\"\\n\")\n",
    "            file.write(\"Lamma-cpp Response time for 3 prompt\\n\")\n",
    "            file.write(f\"Response Time (seconds): {response_time}\\n\")\n",
    "            file.write(f\"CPU Usage (%): {cpu_usage}\\n\")\n",
    "            file.write(f\"Memory Usage (%): {memory_usage}\\n\")\n",
    "            file.write(\"\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while writing to the file: {e}\")\n",
    "        \n",
    "    return response_list, response_time, cpu_usage, memory_usage\n",
    "\n",
    "response = output()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0 (main, Mar  1 2023, 12:49:28) [Clang 14.0.6 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0df86693d46cb4fa165d42afe4c34e25eb5aee45096910808bcdd8709ee78db2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
