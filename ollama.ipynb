{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ctransformars.ipynb  langchain.ipynb\n",
      "lammafileapi.ipynb   ollama.ipynb\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME         \tID          \tSIZE  \tMODIFIED   \n",
      "llama2:latest\t78e26419b446\t3.8 GB\t7 days ago\t\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!ollama list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25ltransferring model data \n",
      "creating model layer ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
      "creating model layer ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
      "creating model layer ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
      "creating model layer ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
      "creating model layer ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
      "creating model layer ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
      "creating model layer ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
      "creating model layer ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
      "creating model layer ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
      "creating model layer ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
      "creating model layer ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
      "creating model layer ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
      "creating model layer ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
      "creating model layer ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
      "creating model layer ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
      "creating model layer ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
      "creating model layer ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
      "creating model layer ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
      "creating model layer ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
      "creating model layer ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
      "creating model layer ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
      "creating model layer ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
      "creating model layer ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
      "creating model layer ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
      "creating model layer ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
      "creating model layer ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
      "creating model layer ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
      "creating model layer ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
      "creating model layer ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
      "creating model layer ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
      "creating model layer ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
      "creating model layer ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
      "creating model layer ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
      "creating model layer ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
      "creating model layer ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
      "creating model layer ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
      "creating model layer ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
      "creating model layer ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
      "creating model layer ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
      "creating model layer ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
      "creating model layer ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
      "creating model layer ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
      "creating model layer ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
      "creating model layer ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
      "creating model layer \n",
      "creating template layer \n",
      "writing layer sha256:6c699d0592086c7352047ce3e2e657acd6390b2cdb445f9a0ee6577742126a6b \n",
      "writing layer sha256:68693db5eb3e0501c644080a545730fc93d2ca2dfddf03633642b99f3a1f0e3c \n",
      "writing layer sha256:f76590d7267697e57b2b71e45657417192b3231955f4f947c76b2da85967300a \n",
      "writing manifest \n",
      "success \u001b[?25h\n"
     ]
    }
   ],
   "source": [
    "! ollama create mistral -f /Users/Tharun/llms/Modelfile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME          \tID          \tSIZE  \tMODIFIED       \n",
      "llama2:latest \t78e26419b446\t3.8 GB\t7 days ago    \t\n",
      "mistral:latest\t56647ec7e641\t2.7 GB\t12 seconds ago\t\n"
     ]
    }
   ],
   "source": [
    "!ollama list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠇ \u001b[?25h\u001b[?25l\u001b[?25l\u001b[2K\u001b[1G\u001b[?25h\u001b[2K\u001b[1G\u001b[?25h\n",
      "\u001b[?25l\u001b[?25hThe\u001b[?25l\u001b[?25h capital\u001b[?25l\u001b[?25h of\u001b[?25l\u001b[?25h Italy\u001b[?25l\u001b[?25h is\u001b[?25l\u001b[?25h Rome\u001b[?25l\u001b[?25h (\u001b[?25l\u001b[?25hR\u001b[?25l\u001b[?25homa\u001b[?25l\u001b[?25h in\u001b[?25l\u001b[?25h Italian\u001b[?25l\u001b[?25h).\u001b[?25l\u001b[?25h\n",
      "\n",
      "\u001b[?25l\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!ollama run llama2:latest \"  capital of italy \"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ollama run llama2:latest \" what is the capital of italy \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['\\x1b[?25l⠙ \\x1b[?25h\\x1b[?25l\\x1b[2K\\x1b[1G⠹ \\x1b[?25h\\x1b[?25l\\x1b[2K\\x1b[1G⠸ \\x1b[?25h\\x1b[?25l\\x1b[2K\\x1b[1G⠼ \\x1b[?25h\\x1b[?25l\\x1b[2K\\x1b[1G⠴ \\x1b[?25h\\x1b[?25l\\x1b[2K\\x1b[1G⠦ \\x1b[?25h\\x1b[?25l\\x1b[2K\\x1b[1G⠧ \\x1b[?25h\\x1b[?25l\\x1b[2K\\x1b[1G⠇ \\x1b[?25h\\x1b[?25l\\x1b[2K\\x1b[1G⠏ \\x1b[?25h\\x1b[?25l\\x1b[2K\\x1b[1G⠋ \\x1b[?25h\\x1b[?25l\\x1b[2K\\x1b[1G⠙ \\x1b[?25h\\x1b[?25l\\x1b[2K\\x1b[1G⠹ \\x1b[?25h\\x1b[?25l\\x1b[2K\\x1b[1G⠸ \\x1b[?25h\\x1b[?25l\\x1b[2K\\x1b[1G⠼ \\x1b[?25h\\x1b[?25l\\x1b[2K\\x1b[1G⠴ \\x1b[?25h\\x1b[?25l\\x1b[2K\\x1b[1G⠦ \\x1b[?25h\\x1b[?25l\\x1b[2K\\x1b[1G⠧ \\x1b[?25h\\x1b[?25l\\x1b[2K\\x1b[1G⠇ \\x1b[?25h\\x1b[?25l\\x1b[2K\\x1b[1G⠏ \\x1b[?25h\\x1b[?25l\\x1b[2K\\x1b[1G⠋ \\x1b[?25h\\x1b[?25l\\x1b[2K\\x1b[1G⠙ \\x1b[?25h\\x1b[?25l\\x1b[2K\\x1b[1G⠹ \\x1b[?25h\\x1b[?25l\\x1b[2K\\x1b[1G⠸ \\x1b[?25h\\x1b[?25l\\x1b[2K\\x1b[1G⠼ \\x1b[?25h\\x1b[?25l\\x1b[2K\\x1b[1G⠴ \\x1b[?25h\\x1b[?25l\\x1b[2K\\x1b[1G⠦ \\x1b[?25h\\x1b[?25l\\x1b[2K\\x1b[1G⠧ \\x1b[?25h\\x1b[?25l\\x1b[2K\\x1b[1G⠇ \\x1b[?25h\\x1b[?25l\\x1b[2K\\x1b[1G⠏ \\x1b[?25h\\x1b[?25l\\x1b[2K\\x1b[1G⠋ \\x1b[?25h\\x1b[?25l\\x1b[2K\\x1b[1G⠙ \\x1b[?25h\\x1b[?25l\\x1b[2K\\x1b[1G⠹ \\x1b[?25h\\x1b[?25l\\x1b[2K\\x1b[1G⠸ \\x1b[?25h\\x1b[?25l\\x1b[2K\\x1b[1G⠼ \\x1b[?25h\\x1b[?25l\\x1b[2K\\x1b[1G⠴ \\x1b[?25h\\x1b[?25l\\x1b[2K\\x1b[1G⠦ \\x1b[?25h\\x1b[?25l\\x1b[2K\\x1b[1G⠧ \\x1b[?25h\\x1b[?25l\\x1b[2K\\x1b[1G⠇ \\x1b[?25h\\x1b[?25l\\x1b[2K\\x1b[1G⠏ \\x1b[?25h\\x1b[?25l\\x1b[2K\\x1b[1G⠋ \\x1b[?25h\\x1b[?25l\\x1b[2K\\x1b[1G⠙ \\x1b[?25h\\x1b[?25l\\x1b[2K\\x1b[1G⠹ \\x1b[?25h\\x1b[?25l\\x1b[2K\\x1b[1G⠸ \\x1b[?25h\\x1b[?25l\\x1b[2K\\x1b[1G⠼ \\x1b[?25h\\x1b[?25l\\x1b[2K\\x1b[1G⠴ \\x1b[?25h\\x1b[?25l\\x1b[2K\\x1b[1G⠦ \\x1b[?25h\\x1b[?25l\\x1b[2K\\x1b[1G⠧ \\x1b[?25h\\x1b[?25l\\x1b[2K\\x1b[1G⠇ \\x1b[?25h\\x1b[?25l\\x1b[2K\\x1b[1G⠏ \\x1b[?25h\\x1b[?25l\\x1b[2K\\x1b[1G⠋ \\x1b[?25h\\x1b[?25l\\x1b[2K\\x1b[1G⠙ \\x1b[?25h\\x1b[?25l\\x1b[2K\\x1b[1G⠹ \\x1b[?25h\\x1b[?25l\\x1b[2K\\x1b[1G⠸ \\x1b[?25h\\x1b[?25l\\x1b[2K\\x1b[1G⠼ \\x1b[?25h\\x1b[?25l\\x1b[2K\\x1b[1G⠴ \\x1b[?25h\\x1b[?25l\\x1b[2K\\x1b[1G⠦ \\x1b[?25h\\x1b[?25l\\x1b[2K\\x1b[1G⠧ \\x1b[?25h\\x1b[?25l\\x1b[2K\\x1b[1G⠇ \\x1b[?25h\\x1b[?25l\\x1b[2K\\x1b[1G⠏ \\x1b[?25h\\x1b[?25l\\x1b[2K\\x1b[1G⠋ \\x1b[?25h\\x1b[?25l\\x1b[2K\\x1b[1G⠙ \\x1b[?25h\\x1b[?25l\\x1b[2K\\x1b[1G⠹ \\x1b[?25h\\x1b[?25l\\x1b[2K\\x1b[1G⠸ \\x1b[?25h\\x1b[?25l\\x1b[2K\\x1b[1G⠼ \\x1b[?25h\\x1b[?25l\\x1b[2K\\x1b[1G⠴ \\x1b[?25h\\x1b[?25l\\x1b[2K\\x1b[1G⠦ \\x1b[?25h\\x1b[?25l\\x1b[2K\\x1b[1G⠧ \\x1b[?25h\\x1b[?25l\\x1b[2K\\x1b[1G⠇ \\x1b[?25h\\x1b[?25l\\x1b[2K\\x1b[1G⠏ \\x1b[?25h\\x1b[?25l\\x1b[2K\\x1b[1G⠋ \\x1b[?25h\\x1b[?25l\\x1b[2K\\x1b[1G⠙ \\x1b[?25h\\x1b[?25l\\x1b[2K\\x1b[1G⠹ \\x1b[?25h\\x1b[?25l\\x1b[2K\\x1b[1G⠸ \\x1b[?25h\\x1b[?25l\\x1b[2K\\x1b[1G⠼ \\x1b[?25h\\x1b[?25l\\x1b[2K\\x1b[1G⠴ \\x1b[?25h\\x1b[?25l\\x1b[2K\\x1b[1G⠦ \\x1b[?25h\\x1b[?25l\\x1b[2K\\x1b[1G⠧ \\x1b[?25h\\x1b[?25l\\x1b[2K\\x1b[1G⠇ \\x1b[?25h\\x1b[?25l\\x1b[2K\\x1b[1G⠏ \\x1b[?25h\\x1b[?25l\\x1b[2K\\x1b[1G⠋ \\x1b[?25h\\x1b[?25l\\x1b[2K\\x1b[1G⠙ \\x1b[?25h\\x1b[?25l\\x1b[2K\\x1b[1G⠹ \\x1b[?25h\\x1b[?25l\\x1b[2K\\x1b[1G⠸ \\x1b[?25h\\x1b[?25l\\x1b[2K\\x1b[1G⠼ \\x1b[?25h\\x1b[?25l\\x1b[2K\\x1b[1G⠴ \\x1b[?25h\\x1b[?25l\\x1b[2K\\x1b[1G⠦ \\x1b[?25h\\x1b[?25l\\x1b[2K\\x1b[1G⠧ \\x1b[?25h\\x1b[?25l\\x1b[2K\\x1b[1G⠇ \\x1b[?25h\\x1b[?25l\\x1b[2K\\x1b[1G⠏ \\x1b[?25h\\x1b[?25l\\x1b[2K\\x1b[1G⠋ \\x1b[?25h\\x1b[?25l\\x1b[2K\\x1b[1G⠏ \\x1b[?25h\\x1b[?25l\\x1b[2K\\x1b[1G⠋ \\x1b[?25h\\x1b[?25l\\x1b[2K\\x1b[1G⠦ \\x1b[?25h\\x1b[?25l\\x1b[2K\\x1b[1G⠦ \\x1b[?25h\\x1b[?25l\\x1b[2K\\x1b[1G⠧ \\x1b[?25h\\x1b[?25l\\x1b[2K\\x1b[1G⠇ \\x1b[?25h\\x1b[?25l\\x1b[2K\\x1b[1G⠋ \\x1b[?25h\\x1b[?25l\\x1b[2K\\x1b[1G⠋ \\x1b[?25h\\x1b[?25l\\x1b[2K\\x1b[1G⠙ \\x1b[?25h\\x1b[?25l\\x1b[2K\\x1b[1G⠹ \\x1b[?25h\\x1b[?25l\\x1b[2K\\x1b[1G⠸ \\x1b[?25h\\x1b[?25l\\x1b[2K\\x1b[1G⠼ \\x1b[?25h\\x1b[?25l\\x1b[?25l\\x1b[2K\\x1b[1G\\x1b[?25h\\x1b[2K\\x1b[1G\\x1b[?25h', '\\x1b[?25l\\x1b[?25hThe\\x1b[?25l\\x1b[?25h capital\\x1b[?25l\\x1b[?25h of\\x1b[?25l\\x1b[?25h Italy\\x1b[?25l\\x1b[?25h is\\x1b[?25l\\x1b[?25h Rome\\x1b[?25l\\x1b[?25h (\\x1b[?25l\\x1b[?25hR\\x1b[?25l\\x1b[?25homa\\x1b[?25l\\x1b[?25h in\\x1b[?25l\\x1b[?25h Italian\\x1b[?25l\\x1b[?25h).\\x1b[?25l\\x1b[?25h', '', '\\x1b[?25l\\x1b[?25h'], 13.084671020507812, 18.1, 99.3)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import psutil\n",
    "\n",
    "\n",
    "\n",
    "def output():\n",
    "    \n",
    "    prompt = \"\"\" Question: capital of italy\"\"\"\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    response = !ollama run llama2:latest \"  capital of italy \"\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "\n",
    "    response_time = end_time - start_time\n",
    "\n",
    "\n",
    "    cpu_usage = psutil.cpu_percent()\n",
    "    \n",
    "    memory_usage = psutil.virtual_memory().percent\n",
    "    \n",
    "    with open(\"resource_usage.txt\", \"a\") as file:\n",
    "        file.write(f\"Response Time (seconds): {response_time}\\n\")\n",
    "        file.write(f\"CPU Usage (%): {cpu_usage}\\n\")\n",
    "        file.write(f\"Memory Usage (%): {memory_usage}\\n\")\n",
    "        file.write(\"\\n\") \n",
    "        \n",
    "    return response, response_time, cpu_usage, memory_usage\n",
    "\n",
    "\n",
    "responce = output()\n",
    "\n",
    "print(responce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['⠙ ⠹ ⠸ ⠼ ⠴ ⠦ ⠧ ⠇ ⠏ ⠋ ⠙ ⠹ ⠸ ⠼ ⠴ ⠦ ⠧ ⠇ ⠏ ⠋ ⠙ ⠹ ⠸ ⠼ ⠴ ⠦ ⠧ ⠇ ⠏ ⠋ ⠙ ⠹ ⠸ ⠼ ⠴ ⠦ ⠧ ⠇ ⠏ ⠋ ⠙ ⠹ ⠸ ⠼ ⠴ ⠦ ⠧ ⠇ ⠏ ⠋ ⠙ ⠹ ⠸ ⠼ ⠴ ⠦ ⠧ ⠇ ⠏ ⠋ ⠙ ⠹ ⠸ ⠼ ⠴ ⠦ ⠧ ⠇ ⠏ ⠋ ⠙ ⠹ ⠸ ⠼ ⠴ ⠦ ⠧ ⠇ ⠏ ⠋ ⠙ ⠹ ⠸ ⠼ ⠴ ⠦ ⠧ ⠇ ⠏ ⠋ ⠙ ⠹ ⠸ ⠼ ⠴ ⠦ ⠧ ⠇ ⠏ ⠋ ⠙ ⠹ ⠸ ⠼ ⠴ ⠦ ⠧ ⠇ ⠏ ⠋ ⠙ ⠹ ⠸ ⠼ ⠴', 'The capital of Italy is Rome (Roma in Italian).', '', ''], 13.41598105430603, 28.9, 99.2)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def output():\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    prompts = [\"What is the capital of italy ?\"]\n",
    "    responce = [ ]\n",
    "    for prompt in prompts:\n",
    "        responce.append(!ollama run llama2:latest prompt\n",
    "        print(f\"Prompt: {prompt}\")\n",
    "        print(f\"Response: {response}\")\n",
    "        print(\"-\" * 20)  # Separator\n",
    "    end_time = time.time()\n",
    "    response_time = end_time - start_time\n",
    "\n",
    "    # Remove escape sequences and strip whitespace from the response\n",
    "    clean_response = [re.sub(r'\\x1b\\[[0-?]*[ -/]*[@-~]', '', line).strip() for line in response]\n",
    "\n",
    "    cpu_usage = psutil.cpu_percent()\n",
    "    \n",
    "    memory_usage = psutil.virtual_memory().percent\n",
    "    \n",
    "    with open(\"resource_usage.txt\", \"a\") as file:\n",
    "        file.write(f\"Response Time (seconds): {response_time}\\n\")\n",
    "        file.write(f\"CPU Usage (%): {cpu_usage}\\n\")\n",
    "        file.write(f\"Memory Usage (%): {memory_usage}\\n\")\n",
    "        file.write(\"\\n\") \n",
    "        \n",
    "    return clean_response, response_time, cpu_usage, memory_usage\n",
    "\n",
    "response = output()\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: What is the capital of Italy?\n",
      "Response: \n",
      "The capital of Italy is Rome (Roma in Italian).\n",
      "\n",
      "\n",
      "--------------------\n",
      "Prompt: Write a 6 line poem.\n",
      "Response: \n",
      "The sun sets slow and paints the sky,\n",
      "A fiery hue that makes me sigh.\n",
      "The stars come out and twinkle bright,\n",
      "A night of peace, a sight so right.\n",
      "The world is quiet, still and deep,\n",
      "A time for dreams, a time to sleep.\n",
      "\n",
      "\n",
      "--------------------\n",
      "Prompt: What are LLM models?\n",
      "Response: \n",
      "LLM (Master of Laws) models refer to the various types of academic programs offered by law schools for students who have already completed their undergraduate degree in law. These programs are designed to provide advanced legal education and training to students who want to specialize in a particular area of law, such as corporate law, intellectual property law, international law, or human rights law.\n",
      "\n",
      "There are several types of LLM models that are offered by law schools around the world:\n",
      "\n",
      "1. Traditional LLM Programs: These programs typically involve a one-year course of study that is similar to a JD program in the United States. Students take courses and participate in clinics, seminars, and other learning activities, and they are required to complete a research paper or project under the supervision of a faculty member.\n",
      "2. Specialized LLM Programs: These programs are designed for students who want to specialize in a particular area of law, such as tax law, securities law, or environmental law. These programs typically involve a more focused course of study than traditional LLM programs and may include coursework, clinics, and other learning activities that are tailored to the student's area of interest.\n",
      "3. Online LLM Programs: These programs are designed for students who cannot attend classes on a traditional campus. They typically involve a combination of online courses, discussion boards, and other interactive learning activities.\n",
      "4. Executive LLM Programs: These programs are designed for professionals who have several years of work experience in the legal field. They typically involve a shorter course of study and may include practical training and networking opportunities.\n",
      "5. Joint Degree Programs: These programs allow students to earn both an LLM degree and another graduate degree, such as an MBA or an MS in a related field, in a shorter period of time than it would take to complete two separate degrees.\n",
      "6. Specialized Certificate Programs: These programs are designed for students who want to gain advanced knowledge and skills in a particular area of law, such as intellectual property or international trade law. They typically involve a smaller course of study than an LLM program and may include coursework, clinics, and other learning activities.\n",
      "7. Distance Learning Programs: These programs are designed for students who cannot attend classes on a traditional campus due to location or time constraints. They typically involve a combination of online courses, discussion boards, and other interactive learning activities.\n",
      "8. Blended Learning Programs: These programs combine traditional classroom instruction with online learning activities. Students have the option to attend classes on campus or participate in online courses, depending on their preferences and schedule.\n",
      "9. Executive Education Programs: These programs are designed for professionals who want to gain advanced knowledge and skills in a particular area of law without pursuing an LLM degree. They typically involve shorter courses of study and may include practical training and networking opportunities.\n",
      "10. Customized Programs: Some law schools offer customized LLM programs that are tailored to the needs and interests of individual students or groups of students. These programs can be designed in a variety of ways, including specialized coursework, research projects, and clinical experiences.\n",
      "\n",
      "\n",
      "--------------------\n",
      "(['The capital of Italy is Rome (Roma in Italian).', 'The sun sets slow and paints the sky,\\nA fiery hue that makes me sigh.\\nThe stars come out and twinkle bright,\\nA night of peace, a sight so right.\\nThe world is quiet, still and deep,\\nA time for dreams, a time to sleep.', \"LLM (Master of Laws) models refer to the various types of academic programs offered by law schools for students who have already completed their undergraduate degree in law. These programs are designed to provide advanced legal education and training to students who want to specialize in a particular area of law, such as corporate law, intellectual property law, international law, or human rights law.\\n\\nThere are several types of LLM models that are offered by law schools around the world:\\n\\n1. Traditional LLM Programs: These programs typically involve a one-year course of study that is similar to a JD program in the United States. Students take courses and participate in clinics, seminars, and other learning activities, and they are required to complete a research paper or project under the supervision of a faculty member.\\n2. Specialized LLM Programs: These programs are designed for students who want to specialize in a particular area of law, such as tax law, securities law, or environmental law. These programs typically involve a more focused course of study than traditional LLM programs and may include coursework, clinics, and other learning activities that are tailored to the student's area of interest.\\n3. Online LLM Programs: These programs are designed for students who cannot attend classes on a traditional campus. They typically involve a combination of online courses, discussion boards, and other interactive learning activities.\\n4. Executive LLM Programs: These programs are designed for professionals who have several years of work experience in the legal field. They typically involve a shorter course of study and may include practical training and networking opportunities.\\n5. Joint Degree Programs: These programs allow students to earn both an LLM degree and another graduate degree, such as an MBA or an MS in a related field, in a shorter period of time than it would take to complete two separate degrees.\\n6. Specialized Certificate Programs: These programs are designed for students who want to gain advanced knowledge and skills in a particular area of law, such as intellectual property or international trade law. They typically involve a smaller course of study than an LLM program and may include coursework, clinics, and other learning activities.\\n7. Distance Learning Programs: These programs are designed for students who cannot attend classes on a traditional campus due to location or time constraints. They typically involve a combination of online courses, discussion boards, and other interactive learning activities.\\n8. Blended Learning Programs: These programs combine traditional classroom instruction with online learning activities. Students have the option to attend classes on campus or participate in online courses, depending on their preferences and schedule.\\n9. Executive Education Programs: These programs are designed for professionals who want to gain advanced knowledge and skills in a particular area of law without pursuing an LLM degree. They typically involve shorter courses of study and may include practical training and networking opportunities.\\n10. Customized Programs: Some law schools offer customized LLM programs that are tailored to the needs and interests of individual students or groups of students. These programs can be designed in a variety of ways, including specialized coursework, research projects, and clinical experiences.\"], 287.2532720565796, 20.9, 99.2)\n"
     ]
    }
   ],
   "source": [
    "# multiple promts \n",
    "\n",
    "import re\n",
    "import subprocess\n",
    "import time\n",
    "import psutil\n",
    "\n",
    "def output():\n",
    "    start_time = time.time()\n",
    "    \n",
    "    prompts = [\"What is the capital of Italy?\", \"Write a 6 line poem.\", \"What are LLM models?\"]\n",
    "    responce = []\n",
    "\n",
    "    for prompt in prompts:\n",
    "        command = f\"ollama run llama2:latest '{prompt}'\"\n",
    "        response = subprocess.run(command, shell=True, capture_output=True, text=True).stdout\n",
    "        responce.append(response.strip())\n",
    "        print(f\"Prompt: {prompt}\")\n",
    "        print(f\"Response: {response}\")\n",
    "        print(\"-\" * 20)  # Separator\n",
    "    \n",
    "    end_time = time.time()\n",
    "    response_time = end_time - start_time\n",
    "\n",
    "    cpu_usage = psutil.cpu_percent()\n",
    "    memory_usage = psutil.virtual_memory().percent\n",
    "    \n",
    "    with open(\"resource_usage.txt\", \"a\") as file:\n",
    "        file.write(f\"Response Time (seconds): {response_time}\\n\")\n",
    "        file.write(f\"CPU Usage (%): {cpu_usage}\\n\")\n",
    "        file.write(f\"Memory Usage (%): {memory_usage}\\n\")\n",
    "        file.write(\"\\n\") \n",
    "        \n",
    "    return responce, response_time, cpu_usage, memory_usage\n",
    "\n",
    "response = output()\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0 (main, Mar  1 2023, 12:49:28) [Clang 14.0.6 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0df86693d46cb4fa165d42afe4c34e25eb5aee45096910808bcdd8709ee78db2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
