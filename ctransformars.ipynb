{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: conda [-h] [--no-plugins] [-V] COMMAND ...\n",
      "conda: error: argument COMMAND: invalid choice: 'activate' (choose from 'clean', 'compare', 'config', 'create', 'info', 'init', 'install', 'list', 'notices', 'package', 'remove', 'uninstall', 'rename', 'run', 'search', 'update', 'upgrade', 'doctor', 'render', 'skeleton', 'token', 'server', 'develop', 'build', 'debug', 'convert', 'index', 'pack', 'content-trust', 'verify', 'inspect', 'repo', 'metapackage', 'env')\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda activate llms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/Tharun/opt/anaconda3/envs/llms/lib/python3.11/site-packages (2.2.1)\n",
      "Requirement already satisfied: filelock in /Users/Tharun/opt/anaconda3/envs/llms/lib/python3.11/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/Tharun/opt/anaconda3/envs/llms/lib/python3.11/site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in /Users/Tharun/opt/anaconda3/envs/llms/lib/python3.11/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/Tharun/opt/anaconda3/envs/llms/lib/python3.11/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/Tharun/opt/anaconda3/envs/llms/lib/python3.11/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /Users/Tharun/opt/anaconda3/envs/llms/lib/python3.11/site-packages (from torch) (2024.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/Tharun/opt/anaconda3/envs/llms/lib/python3.11/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/Tharun/opt/anaconda3/envs/llms/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ctransformers in /Users/Tharun/opt/anaconda3/envs/llms/lib/python3.11/site-packages (0.2.27)\n",
      "Requirement already satisfied: huggingface-hub in /Users/Tharun/opt/anaconda3/envs/llms/lib/python3.11/site-packages (from ctransformers) (0.21.3)\n",
      "Requirement already satisfied: py-cpuinfo<10.0.0,>=9.0.0 in /Users/Tharun/opt/anaconda3/envs/llms/lib/python3.11/site-packages (from ctransformers) (9.0.0)\n",
      "Requirement already satisfied: filelock in /Users/Tharun/opt/anaconda3/envs/llms/lib/python3.11/site-packages (from huggingface-hub->ctransformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/Tharun/opt/anaconda3/envs/llms/lib/python3.11/site-packages (from huggingface-hub->ctransformers) (2024.2.0)\n",
      "Requirement already satisfied: requests in /Users/Tharun/opt/anaconda3/envs/llms/lib/python3.11/site-packages (from huggingface-hub->ctransformers) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /Users/Tharun/opt/anaconda3/envs/llms/lib/python3.11/site-packages (from huggingface-hub->ctransformers) (4.66.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/Tharun/opt/anaconda3/envs/llms/lib/python3.11/site-packages (from huggingface-hub->ctransformers) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/Tharun/opt/anaconda3/envs/llms/lib/python3.11/site-packages (from huggingface-hub->ctransformers) (4.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/Tharun/opt/anaconda3/envs/llms/lib/python3.11/site-packages (from huggingface-hub->ctransformers) (23.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/Tharun/opt/anaconda3/envs/llms/lib/python3.11/site-packages (from requests->huggingface-hub->ctransformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/Tharun/opt/anaconda3/envs/llms/lib/python3.11/site-packages (from requests->huggingface-hub->ctransformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/Tharun/opt/anaconda3/envs/llms/lib/python3.11/site-packages (from requests->huggingface-hub->ctransformers) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/Tharun/opt/anaconda3/envs/llms/lib/python3.11/site-packages (from requests->huggingface-hub->ctransformers) (2024.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pip install ctransformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.38.2-py3-none-any.whl.metadata (130 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.7/130.7 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /Users/Tharun/opt/anaconda3/envs/llms/lib/python3.11/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /Users/Tharun/opt/anaconda3/envs/llms/lib/python3.11/site-packages (from transformers) (0.21.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/Tharun/opt/anaconda3/envs/llms/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/Tharun/opt/anaconda3/envs/llms/lib/python3.11/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/Tharun/opt/anaconda3/envs/llms/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2023.12.25-cp311-cp311-macosx_10_9_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /Users/Tharun/opt/anaconda3/envs/llms/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers)\n",
      "  Downloading tokenizers-0.15.2-cp311-cp311-macosx_10_12_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.2-cp311-cp311-macosx_10_12_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/Tharun/opt/anaconda3/envs/llms/lib/python3.11/site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/Tharun/opt/anaconda3/envs/llms/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/Tharun/opt/anaconda3/envs/llms/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/Tharun/opt/anaconda3/envs/llms/lib/python3.11/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/Tharun/opt/anaconda3/envs/llms/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/Tharun/opt/anaconda3/envs/llms/lib/python3.11/site-packages (from requests->transformers) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/Tharun/opt/anaconda3/envs/llms/lib/python3.11/site-packages (from requests->transformers) (2024.2.2)\n",
      "Downloading transformers-4.38.2-py3-none-any.whl (8.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2023.12.25-cp311-cp311-macosx_10_9_x86_64.whl (296 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.5/296.5 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.2-cp311-cp311-macosx_10_12_x86_64.whl (426 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m426.3/426.3 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.15.2-cp311-cp311-macosx_10_12_x86_64.whl (2.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: safetensors, regex, tokenizers, transformers\n",
      "Successfully installed regex-2023.12.25 safetensors-0.4.2 tokenizers-0.15.2 transformers-4.38.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ctransformers import AutoModelForCausalLM\n",
    "\n",
    "llm2 = AutoModelForCausalLM.from_pretrained(\"capybarahermes-2.5-mistral-7b.Q2_K.gguf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m llm2(\u001b[39m\"\u001b[39m\u001b[39mwhat is the capital of italy \u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/llms/lib/python3.11/site-packages/ctransformers/llm.py:664\u001b[0m, in \u001b[0;36mLLM.__call__\u001b[0;34m(self, prompt, max_new_tokens, top_k, top_p, temperature, repetition_penalty, last_n_tokens, seed, batch_size, threads, stop, stream, reset)\u001b[0m\n\u001b[1;32m    662\u001b[0m \u001b[39mif\u001b[39;00m stream:\n\u001b[1;32m    663\u001b[0m     \u001b[39mreturn\u001b[39;00m text\n\u001b[0;32m--> 664\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(text)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/llms/lib/python3.11/site-packages/ctransformers/llm.py:570\u001b[0m, in \u001b[0;36mLLM._stream\u001b[0;34m(self, prompt, max_new_tokens, top_k, top_p, temperature, repetition_penalty, last_n_tokens, seed, batch_size, threads, stop, reset)\u001b[0m\n\u001b[1;32m    568\u001b[0m text \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m incomplete \u001b[39m=\u001b[39m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 570\u001b[0m \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenerate(\n\u001b[1;32m    571\u001b[0m     tokens,\n\u001b[1;32m    572\u001b[0m     top_k\u001b[39m=\u001b[39mtop_k,\n\u001b[1;32m    573\u001b[0m     top_p\u001b[39m=\u001b[39mtop_p,\n\u001b[1;32m    574\u001b[0m     temperature\u001b[39m=\u001b[39mtemperature,\n\u001b[1;32m    575\u001b[0m     repetition_penalty\u001b[39m=\u001b[39mrepetition_penalty,\n\u001b[1;32m    576\u001b[0m     last_n_tokens\u001b[39m=\u001b[39mlast_n_tokens,\n\u001b[1;32m    577\u001b[0m     seed\u001b[39m=\u001b[39mseed,\n\u001b[1;32m    578\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[1;32m    579\u001b[0m     threads\u001b[39m=\u001b[39mthreads,\n\u001b[1;32m    580\u001b[0m     reset\u001b[39m=\u001b[39mreset,\n\u001b[1;32m    581\u001b[0m ):\n\u001b[1;32m    582\u001b[0m     \u001b[39m# Handle incomplete UTF-8 multi-byte characters.\u001b[39;00m\n\u001b[1;32m    583\u001b[0m     incomplete \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdetokenize([token], decode\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    584\u001b[0m     complete, incomplete \u001b[39m=\u001b[39m utf8_split_incomplete(incomplete)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/llms/lib/python3.11/site-packages/ctransformers/llm.py:537\u001b[0m, in \u001b[0;36mLLM.generate\u001b[0;34m(self, tokens, top_k, top_p, temperature, repetition_penalty, last_n_tokens, seed, batch_size, threads, reset)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    529\u001b[0m     token \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msample(\n\u001b[1;32m    530\u001b[0m         top_k\u001b[39m=\u001b[39mtop_k,\n\u001b[1;32m    531\u001b[0m         top_p\u001b[39m=\u001b[39mtop_p,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    535\u001b[0m         seed\u001b[39m=\u001b[39mseed,\n\u001b[1;32m    536\u001b[0m     )\n\u001b[0;32m--> 537\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39meval([token], batch_size\u001b[39m=\u001b[39mbatch_size, threads\u001b[39m=\u001b[39mthreads)\n\u001b[1;32m    538\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_eos_token(token):\n\u001b[1;32m    539\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/llms/lib/python3.11/site-packages/ctransformers/llm.py:403\u001b[0m, in \u001b[0;36mLLM.eval\u001b[0;34m(self, tokens, batch_size, threads)\u001b[0m\n\u001b[1;32m    399\u001b[0m     logger\u001b[39m.\u001b[39mwarning(\n\u001b[1;32m    400\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNumber of tokens (\u001b[39m\u001b[39m{\u001b[39;00mn_past\u001b[39m \u001b[39m\u001b[39m+\u001b[39m\u001b[39m \u001b[39mn_tokens\u001b[39m}\u001b[39;00m\u001b[39m) exceeded maximum context length (\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontext_length\u001b[39m}\u001b[39;00m\u001b[39m).\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    401\u001b[0m     )\n\u001b[1;32m    402\u001b[0m tokens \u001b[39m=\u001b[39m (c_int \u001b[39m*\u001b[39m n_tokens)(\u001b[39m*\u001b[39mtokens)\n\u001b[0;32m--> 403\u001b[0m status \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mctransformers_llm_batch_eval(\n\u001b[1;32m    404\u001b[0m     tokens,\n\u001b[1;32m    405\u001b[0m     n_tokens,\n\u001b[1;32m    406\u001b[0m     n_past,\n\u001b[1;32m    407\u001b[0m     batch_size,\n\u001b[1;32m    408\u001b[0m     threads,\n\u001b[1;32m    409\u001b[0m )\n\u001b[1;32m    410\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m status:\n\u001b[1;32m    411\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mFailed to evaluate tokens.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "llm2(\"what is the capital of italy \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single  promt \n",
    "\n",
    "import re\n",
    "import subprocess\n",
    "import time\n",
    "import psutil\n",
    "\n",
    "def output():\n",
    "    start_time = time.time()\n",
    "    \n",
    "    prompts = [\"What is the capital of Italy?\"]\n",
    "    responce = []\n",
    "\n",
    "    for prompt in prompts:\n",
    "        response = llm2(prompt)\n",
    "        responce.append(response.strip())\n",
    "        print(f\"Prompt: {prompt}\")\n",
    "        print(f\"Response: {response}\")\n",
    "        print(\"-\" * 20)  # Separator\n",
    "    \n",
    "    end_time = time.time()\n",
    "    response_time = end_time - start_time\n",
    "\n",
    "    cpu_usage = psutil.cpu_percent()\n",
    "    memory_usage = psutil.virtual_memory().percent\n",
    "    \n",
    "    with open(\"resource_usage.txt\", \"a\") as file:\n",
    "            file.write(\"\\n\")\n",
    "            file.write(\"Langchain Response time for 1 prompt\\n\")\n",
    "            file.write(f\"Response Time (seconds): {response_time}\\n\")\n",
    "            file.write(f\"CPU Usage (%): {cpu_usage}\\n\")\n",
    "            file.write(f\"Memory Usage (%): {memory_usage}\\n\")\n",
    "            file.write(\"\\n\") \n",
    "        \n",
    "    return responce, response_time, cpu_usage, memory_usage\n",
    "\n",
    "response = output()\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also use local models using ctransformers \n",
    "from ctransformers import AutoModelForCausalLM\n",
    "\n",
    "\n",
    "llm2 = AutoModelForCausalLM.from_pretrained(\"/Users/Tharun/llms/capybarahermes-2.5-mistral-7b.Q2_K.gguf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import psutil\n",
    "from ctransformers import AutoModelForCausalLM\n",
    "\n",
    "\n",
    "\n",
    "def output():\n",
    "    \n",
    "    prompt = \"\"\" Question: capital of italy\"\"\"\n",
    "\n",
    "    start_time = time.time()\n",
    "    llm1 = AutoModelForCausalLM.from_pretrained(\"/Users/Tharun/llms/capybarahermes-2.5-mistral-7b.Q2_K.gguf\")\n",
    "    \n",
    "    response = llm1(\" capital of italy \")\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "\n",
    "    response_time = end_time - start_time\n",
    "\n",
    "\n",
    "    cpu_usage = psutil.cpu_percent()\n",
    "    \n",
    "    memory_usage = psutil.virtual_memory().percent\n",
    "    \n",
    "    with open(\"resource_usage.txt\", \"a\") as file:\n",
    "        file.write(f\" ctransformers responce time for single promt \")\n",
    "        file.write(f\"Response Time (seconds): {response_time}\\n\")\n",
    "        file.write(f\"CPU Usage (%): {cpu_usage}\\n\")\n",
    "        file.write(f\"Memory Usage (%): {memory_usage}\\n\")\n",
    "        file.write(\"\\n\") \n",
    "        \n",
    "    return response, response_time, cpu_usage, memory_usage\n",
    "\n",
    "\n",
    "responce = output()\n",
    "\n",
    "print(responce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\", padding_side='left')\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
    "\n",
    "# source: https://huggingface.co/microsoft/DialoGPT-medium\n",
    "\n",
    "# Let's chat for 5 lines\n",
    "for step in range(5):\n",
    "    # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
    "    new_user_input_ids = tokenizer.encode(input(\">> User:\") + tokenizer.eos_token, return_tensors='pt')\n",
    "\n",
    "    # append the new user input tokens to the chat history\n",
    "    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n",
    "\n",
    "    # generated a response while limiting the total chat history to 1000 tokens, \n",
    "    chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[39m# Entry point of the script\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m---> 20\u001b[0m     main()\n",
      "Cell \u001b[0;32mIn[4], line 10\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m model_location \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/Users/Tharun/llms/capybarahermes-2.5-mistral-7b.Q4_K_M.gguf\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[39m# Interact with the local model and get response, response time, and resource consumption\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m response_text, response_time, cpu_usage, memory_usage \u001b[39m=\u001b[39m interact_with_local_model(prompt, model_location, local_model)\n\u001b[1;32m     12\u001b[0m \u001b[39m# Print the response, response time, and resource consumption\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mModel Response:\u001b[39m\u001b[39m\"\u001b[39m, response_text)\n",
      "Cell \u001b[0;32mIn[3], line 11\u001b[0m, in \u001b[0;36minteract_with_local_model\u001b[0;34m(prompt, model_location, local_model)\u001b[0m\n\u001b[1;32m      8\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m     10\u001b[0m \u001b[39m# Interaction with the local model (replace this with your actual interaction code)\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m response \u001b[39m=\u001b[39m local_model(prompt, model_location)\n\u001b[1;32m     13\u001b[0m \u001b[39m# Record end time\u001b[39;00m\n\u001b[1;32m     14\u001b[0m end_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n",
      "Cell \u001b[0;32mIn[3], line 31\u001b[0m, in \u001b[0;36mlocal_model\u001b[0;34m(prompt, model_location)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlocal_model\u001b[39m(prompt,model_location):\n\u001b[1;32m     27\u001b[0m     \u001b[39m# Replace this with your actual interaction with the locally hosted model\u001b[39;00m\n\u001b[1;32m     28\u001b[0m     \u001b[39m# Here, we're simply returning a placeholder response\u001b[39;00m\n\u001b[1;32m     29\u001b[0m     llm \u001b[39m=\u001b[39m AutoModelForCausalLM\u001b[39m.\u001b[39mfrom_pretrained(model_location)\n\u001b[0;32m---> 31\u001b[0m     response \u001b[39m=\u001b[39m llm(prompt)\n\u001b[1;32m     33\u001b[0m     \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/llms/lib/python3.11/site-packages/ctransformers/llm.py:664\u001b[0m, in \u001b[0;36mLLM.__call__\u001b[0;34m(self, prompt, max_new_tokens, top_k, top_p, temperature, repetition_penalty, last_n_tokens, seed, batch_size, threads, stop, stream, reset)\u001b[0m\n\u001b[1;32m    662\u001b[0m \u001b[39mif\u001b[39;00m stream:\n\u001b[1;32m    663\u001b[0m     \u001b[39mreturn\u001b[39;00m text\n\u001b[0;32m--> 664\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(text)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/llms/lib/python3.11/site-packages/ctransformers/llm.py:570\u001b[0m, in \u001b[0;36mLLM._stream\u001b[0;34m(self, prompt, max_new_tokens, top_k, top_p, temperature, repetition_penalty, last_n_tokens, seed, batch_size, threads, stop, reset)\u001b[0m\n\u001b[1;32m    568\u001b[0m text \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m incomplete \u001b[39m=\u001b[39m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 570\u001b[0m \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenerate(\n\u001b[1;32m    571\u001b[0m     tokens,\n\u001b[1;32m    572\u001b[0m     top_k\u001b[39m=\u001b[39mtop_k,\n\u001b[1;32m    573\u001b[0m     top_p\u001b[39m=\u001b[39mtop_p,\n\u001b[1;32m    574\u001b[0m     temperature\u001b[39m=\u001b[39mtemperature,\n\u001b[1;32m    575\u001b[0m     repetition_penalty\u001b[39m=\u001b[39mrepetition_penalty,\n\u001b[1;32m    576\u001b[0m     last_n_tokens\u001b[39m=\u001b[39mlast_n_tokens,\n\u001b[1;32m    577\u001b[0m     seed\u001b[39m=\u001b[39mseed,\n\u001b[1;32m    578\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[1;32m    579\u001b[0m     threads\u001b[39m=\u001b[39mthreads,\n\u001b[1;32m    580\u001b[0m     reset\u001b[39m=\u001b[39mreset,\n\u001b[1;32m    581\u001b[0m ):\n\u001b[1;32m    582\u001b[0m     \u001b[39m# Handle incomplete UTF-8 multi-byte characters.\u001b[39;00m\n\u001b[1;32m    583\u001b[0m     incomplete \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdetokenize([token], decode\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    584\u001b[0m     complete, incomplete \u001b[39m=\u001b[39m utf8_split_incomplete(incomplete)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/llms/lib/python3.11/site-packages/ctransformers/llm.py:537\u001b[0m, in \u001b[0;36mLLM.generate\u001b[0;34m(self, tokens, top_k, top_p, temperature, repetition_penalty, last_n_tokens, seed, batch_size, threads, reset)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    529\u001b[0m     token \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msample(\n\u001b[1;32m    530\u001b[0m         top_k\u001b[39m=\u001b[39mtop_k,\n\u001b[1;32m    531\u001b[0m         top_p\u001b[39m=\u001b[39mtop_p,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    535\u001b[0m         seed\u001b[39m=\u001b[39mseed,\n\u001b[1;32m    536\u001b[0m     )\n\u001b[0;32m--> 537\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39meval([token], batch_size\u001b[39m=\u001b[39mbatch_size, threads\u001b[39m=\u001b[39mthreads)\n\u001b[1;32m    538\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_eos_token(token):\n\u001b[1;32m    539\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/llms/lib/python3.11/site-packages/ctransformers/llm.py:403\u001b[0m, in \u001b[0;36mLLM.eval\u001b[0;34m(self, tokens, batch_size, threads)\u001b[0m\n\u001b[1;32m    399\u001b[0m     logger\u001b[39m.\u001b[39mwarning(\n\u001b[1;32m    400\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNumber of tokens (\u001b[39m\u001b[39m{\u001b[39;00mn_past\u001b[39m \u001b[39m\u001b[39m+\u001b[39m\u001b[39m \u001b[39mn_tokens\u001b[39m}\u001b[39;00m\u001b[39m) exceeded maximum context length (\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontext_length\u001b[39m}\u001b[39;00m\u001b[39m).\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    401\u001b[0m     )\n\u001b[1;32m    402\u001b[0m tokens \u001b[39m=\u001b[39m (c_int \u001b[39m*\u001b[39m n_tokens)(\u001b[39m*\u001b[39mtokens)\n\u001b[0;32m--> 403\u001b[0m status \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mctransformers_llm_batch_eval(\n\u001b[1;32m    404\u001b[0m     tokens,\n\u001b[1;32m    405\u001b[0m     n_tokens,\n\u001b[1;32m    406\u001b[0m     n_past,\n\u001b[1;32m    407\u001b[0m     batch_size,\n\u001b[1;32m    408\u001b[0m     threads,\n\u001b[1;32m    409\u001b[0m )\n\u001b[1;32m    410\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m status:\n\u001b[1;32m    411\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mFailed to evaluate tokens.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Main function to run the script\n",
    "def main():\n",
    "    # Define your prompt\n",
    "    prompt = \"what is the capital of italy \"\n",
    "    \n",
    "    # Define the location of your local model\n",
    "    model_location = \"/Users/Tharun/llms/capybarahermes-2.5-mistral-7b.Q4_K_M.gguf\"\n",
    "\n",
    "    # Interact with the local model and get response, response time, and resource consumption\n",
    "    response_text, response_time, cpu_usage, memory_usage = interact_with_local_model(prompt, model_location, local_model)\n",
    "\n",
    "    # Print the response, response time, and resource consumption\n",
    "    print(\"Model Response:\", response_text)\n",
    "    print(\"Response Time (seconds):\", response_time)\n",
    "    print(\"CPU Usage (%):\", cpu_usage)\n",
    "    print(\"Memory Usage (%):\", memory_usage)\n",
    "\n",
    "# Entry point of the script\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "source": [
    "## Transformers pros:\n",
    "\n",
    "## Automatic model downloads\n",
    "## Code snippets available\n",
    "## Ideal for experimentation \n",
    "\n",
    "## Transformers cons:\n",
    "\n",
    "# responce very slow  \n",
    "## Coding and configuration skills are necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0 (main, Mar  1 2023, 12:49:28) [Clang 14.0.6 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0df86693d46cb4fa165d42afe4c34e25eb5aee45096910808bcdd8709ee78db2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
